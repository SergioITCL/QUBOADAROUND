{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as ns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from pickle import dump\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pylab as pl\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data=tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels)=mnist_data.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape\n",
    "np.amax(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Supongamos que tienes tus datos de entrenamiento y prueba: train_images y test_images\\n\\n# Reescalar los datos entre 0 y 1\\ntrain_images_scaled = train_images / 255.0\\ntest_images_scaled = test_images / 255.0\\n\\n# Redimensionar las imágenes a 7x7\\ntrain_images_scaled = np.array([tf.image.resize(np.expand_dims(image, axis=-1), (7, 7)).numpy() for image in train_images_scaled])\\ntest_images_scaled = np.array([tf.image.resize(np.expand_dims(image, axis=-1), (7, 7)).numpy() for image in test_images_scaled])\\n\\n# Aplanar las imágenes\\ntrain_images_scaled = train_images_scaled.reshape(train_images_scaled.shape[0], -1)\\ntest_images_scaled = test_images_scaled.reshape(test_images_scaled.shape[0], -1)\\n\\n# Reescalar los datos entre -1 y 1\\ntrain_images_scaled = 2 * train_images_scaled / np.max(train_images_scaled) - 1\\ntest_images_scaled = 2 * test_images_scaled / np.max(test_images_scaled) - 1\\n\\nprint(\"Tamaño de test_images_scaled:\", test_images_scaled.shape)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "'''\n",
    "# Supongamos que tienes tus datos de entrenamiento y prueba: train_images y test_images\n",
    "\n",
    "# Reescalar los datos entre 0 y 1\n",
    "train_images_scaled = train_images / 255.0\n",
    "test_images_scaled = test_images / 255.0\n",
    "\n",
    "# Redimensionar las imágenes a 7x7\n",
    "train_images_scaled = np.array([tf.image.resize(np.expand_dims(image, axis=-1), (7, 7)).numpy() for image in train_images_scaled])\n",
    "test_images_scaled = np.array([tf.image.resize(np.expand_dims(image, axis=-1), (7, 7)).numpy() for image in test_images_scaled])\n",
    "\n",
    "# Aplanar las imágenes\n",
    "train_images_scaled = train_images_scaled.reshape(train_images_scaled.shape[0], -1)\n",
    "test_images_scaled = test_images_scaled.reshape(test_images_scaled.shape[0], -1)\n",
    "\n",
    "# Reescalar los datos entre -1 y 1\n",
    "train_images_scaled = 2 * train_images_scaled / np.max(train_images_scaled) - 1\n",
    "test_images_scaled = 2 * test_images_scaled / np.max(test_images_scaled) - 1\n",
    "\n",
    "print(\"Tamaño de test_images_scaled:\", test_images_scaled.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 196)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_images_scaled=train_images/255\n",
    "test_images_scaled=test_images/255\n",
    "n=14\n",
    "train_images_scaled = np.array([tf.image.resize(np.expand_dims(image, axis=-1), (n, n)).numpy() for image in train_images_scaled])\n",
    "test_images_scaled = np.array([tf.image.resize(np.expand_dims(image, axis=-1), (n, n)).numpy() for image in test_images_scaled])\n",
    "train_images_scaled = train_images_scaled.reshape(train_images_scaled.shape[0], -1)\n",
    "test_images_scaled = test_images_scaled.reshape(test_images_scaled.shape[0], -1)\n",
    "print(test_images_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 196)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(10, activation='softmax')\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (60000, 10)               1970      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,970\n",
      "Trainable params: 1,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=train_images_scaled.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (60000, 10)               1970      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,970\n",
      "Trainable params: 1,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.7894 - accuracy: 0.8185\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3999 - accuracy: 0.8949\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3450 - accuracy: 0.9053\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3213 - accuracy: 0.9105\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3080 - accuracy: 0.9141\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_images_scaled, train_labels, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/model.h5')\n",
    "# Save the numpy array\n",
    "# Values of X's\n",
    "\n",
    "np.save('X_test.npy', np.array(test_images_scaled))\n",
    "\n",
    "\n",
    "# Values of y's\n",
    "\n",
    "np.save('y_test.npy', np.array(test_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
